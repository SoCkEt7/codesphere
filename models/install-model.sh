#!/bin/bash
# Script to download and set up the CodeLlama-34b-Instruct model

# ANSI color codes
RED='\033[0;31m'
GREEN='\033[0;32m'
YELLOW='\033[0;33m'
BLUE='\033[0;34m'
CYAN='\033[0;36m'
NC='\033[0m' # No Color

# Base directory
BASE_DIR=$(dirname "$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)")
MODELS_DIR="$BASE_DIR/models"
DOWNLOADS_DIR="$MODELS_DIR/downloads"

echo -e "${CYAN}CodeLlama-34b-Instruct Model Installation${NC}"
echo -e "${CYAN}=====================================${NC}"
echo

# Check if Node.js is installed
if ! command -v node &> /dev/null; then
    echo -e "${RED}Error: Node.js is required but not installed.${NC}"
    echo -e "Please install Node.js and npm before running this script."
    exit 1
fi

# Check if npm is installed
if ! command -v npm &> /dev/null; then
    echo -e "${RED}Error: npm is required but not installed.${NC}"
    echo -e "Please install npm before running this script."
    exit 1
fi

# Create directories if they don't exist
mkdir -p "$DOWNLOADS_DIR"
mkdir -p "$MODELS_DIR/config"

# Install required Node.js packages
echo -e "${BLUE}Installing required Node.js packages...${NC}"
npm install --prefix "$BASE_DIR" llama-node @llama-node/core @llama-node/llama-cpp

# Download model weights (This is a simulation - in reality you would download the actual model)
echo -e "${BLUE}Simulating download of CodeLlama-34b-Instruct model...${NC}"
echo "This would normally download the model weights (5-10GB file)"
echo "For this demo, we'll create a placeholder file"

# Create placeholder model file
touch "$DOWNLOADS_DIR/codellama-34b-instruct.gguf"
echo -e "${GREEN}✓${NC} Created model placeholder at $DOWNLOADS_DIR/codellama-34b-instruct.gguf"

# Create model configuration file
echo -e "${BLUE}Creating model configuration...${NC}"

cat > "$MODELS_DIR/config/model-config.json" << EOF
{
  "models": [
    {
      "name": "CodeLlama-34b-Instruct",
      "path": "$DOWNLOADS_DIR/codellama-34b-instruct.gguf",
      "description": "Large model for complex code generation",
      "context_length": 8192,
      "size": "large",
      "parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repetition_penalty": 1.1
      }
    },
    {
      "name": "StarCoder-15b",
      "path": "$DOWNLOADS_DIR/starcoder-15b.gguf",
      "description": "Efficient model for most programming tasks",
      "context_length": 4096,
      "size": "medium",
      "parameters": {
        "temperature": 0.8,
        "top_p": 0.95,
        "top_k": 50,
        "repetition_penalty": 1.0
      }
    },
    {
      "name": "Mistral-7b-Instruct",
      "path": "$DOWNLOADS_DIR/mistral-7b-instruct.gguf",
      "description": "Fast model for simpler tasks",
      "context_length": 4096,
      "size": "small",
      "parameters": {
        "temperature": 0.7,
        "top_p": 0.9,
        "top_k": 40,
        "repetition_penalty": 1.1
      }
    }
  ],
  "default_model": "CodeLlama-34b-Instruct",
  "model_loader": {
    "type": "llama-node",
    "version": "0.1.0"
  }
}
EOF

echo -e "${GREEN}✓${NC} Created model configuration file"

# Create server script for model integration
echo -e "${BLUE}Setting up model server integration...${NC}"

cat > "$MODELS_DIR/server.js" << EOF
/**
 * Model Server for Codesphere
 * Provides local inference using LLaMA models
 */
const { LLM } = require("llama-node");
const { LLamaCpp } = require("llama-node/dist/llm/llama-cpp.js");
const fs = require('fs');
const path = require('path');

// Load configuration
const CONFIG_PATH = path.join(__dirname, 'config/model-config.json');
const config = JSON.parse(fs.readFileSync(CONFIG_PATH, 'utf8'));

// Initialize the model
console.log("Initializing LLM model (simulation)...");

// This would normally initialize the actual model
// const model = new LLM(LLamaCpp);
// const modelPath = config.models[0].path;
// model.load({
//     modelPath,
//     contextSize: config.models[0].context_length,
//     threads: Math.max(1, require('os').cpus().length - 1)
// });

// Simulated model responses for different languages
const responses = {
    javascript: \`/**
 * Simple Express server with two routes
 * Generated by CodeLlama-34b-Instruct
 */
const express = require('express');
const app = express();
const port = process.env.PORT || 3000;

// Middleware to parse JSON in request body
app.use(express.json());

// Route 1: Hello World
app.get('/', (req, res) => {
  res.json({ message: 'Hello World!' });
});

// Route 2: Echo request body
app.post('/echo', (req, res) => {
  res.json({
    echo: req.body,
    timestamp: new Date().toISOString()
  });
});

// Start the server
app.listen(port, () => {
  console.log(\\\`Server running at http://localhost:\\\${port}/\\\`);
});

module.exports = app; // Export for testing\`,

    python: \`#!/usr/bin/env python3
"""
Simple Flask server with two routes
Generated by CodeLlama-34b-Instruct
"""
from flask import Flask, request, jsonify
from datetime import datetime

app = Flask(__name__)

@app.route('/')
def hello_world():
    """Root endpoint returns a greeting"""
    return jsonify({"message": "Hello World!"})

@app.route('/echo', methods=['POST'])
def echo():
    """Echo endpoint returns the request body"""
    data = request.get_json()
    return jsonify({
        "echo": data,
        "timestamp": datetime.now().isoformat()
    })

if __name__ == '__main__':
    app.run(debug=True, host='0.0.0.0', port=5000)
\`,

    html: \`<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>Simple Landing Page</title>
    <style>
        body {
            font-family: 'Arial', sans-serif;
            line-height: 1.6;
            margin: 0;
            padding: 0;
            color: #333;
        }
        .container {
            width: 80%;
            max-width: 1200px;
            margin: 0 auto;
            padding: 2rem;
        }
        header {
            background-color: #4a6baf;
            color: white;
            padding: 1rem 0;
            text-align: center;
        }
        .hero {
            background-color: #f5f5f5;
            padding: 3rem 0;
            text-align: center;
        }
        .features {
            display: flex;
            flex-wrap: wrap;
            justify-content: space-between;
            margin: 2rem 0;
        }
        .feature {
            flex: 1 1 30%;
            margin: 1rem;
            padding: 1.5rem;
            background-color: #fff;
            border-radius: 5px;
            box-shadow: 0 2px 5px rgba(0,0,0,0.1);
        }
        .cta {
            background-color: #4a6baf;
            color: white;
            padding: 1rem 2rem;
            border: none;
            border-radius: 4px;
            font-size: 1.1rem;
            cursor: pointer;
        }
        footer {
            background-color: #333;
            color: white;
            text-align: center;
            padding: 1.5rem 0;
            margin-top: 2rem;
        }
    </style>
</head>
<body>
    <header>
        <div class="container">
            <h1>My Awesome Product</h1>
            <p>The solution you've been waiting for</p>
        </div>
    </header>
    
    <section class="hero">
        <div class="container">
            <h2>Welcome to the Future</h2>
            <p>Our product helps you achieve amazing results with minimal effort</p>
            <button class="cta">Get Started</button>
        </div>
    </section>
    
    <section class="container">
        <h2>Key Features</h2>
        <div class="features">
            <div class="feature">
                <h3>Easy to Use</h3>
                <p>Simple interface that anyone can master in minutes</p>
            </div>
            <div class="feature">
                <h3>Powerful</h3>
                <p>Advanced functionality for when you need more</p>
            </div>
            <div class="feature">
                <h3>Reliable</h3>
                <p>99.9% uptime guarantee so you never miss a beat</p>
            </div>
        </div>
    </section>
    
    <footer>
        <div class="container">
            <p>&copy; 2025 My Awesome Company. All rights reserved.</p>
        </div>
    </footer>
</body>
</html>\`
};

// Simulate model inference
function generateResponse(prompt, language) {
    // In a real implementation, this would call the model
    // return await model.complete({
    //    prompt: prompt,
    //    maxTokens: 2048,
    //    temperature: config.models[0].parameters.temperature,
    //    topP: config.models[0].parameters.top_p,
    //    topK: config.models[0].parameters.top_k,
    //    repeatPenalty: config.models[0].parameters.repetition_penalty
    // });
    
    // For simulation, return pre-written responses
    return responses[language] || 
        \`// Generated code for \${language} would appear here.\n// Based on: \${prompt}\`;
}

// Export for use in main application
module.exports = { generateResponse };

// For testing the server independently
if (require.main === module) {
    console.log("Model server initialized and ready");
    const testPrompt = "Create a simple Express server with two routes";
    console.log("Test generation:", generateResponse(testPrompt, "javascript").substring(0, 100) + "...");
}
EOF

echo -e "${GREEN}✓${NC} Created model server script"

# Make the script executable
chmod +x "$MODELS_DIR/server.js"

echo
echo -e "${GREEN}Installation complete!${NC}"
echo "The CodeLlama-34b-Instruct model has been set up."
echo "Integration with Codesphere is ready."
echo

exit 0